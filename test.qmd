---
title: "Stat 244-SC"
---

Attached is my final project from my STAT 244-Statistical Computation course at Mount Holyoke College.

## Introduction

This project investigates the factors that influence a newborn's weight using a dataset that includes information on parental age, smoking status, baby sex, and whether the birth was premature. My initial goal was to build a predictive model for infant birth weight using multiple linear regression and cross-validation. I also came to apply unsupervised learning (K-means clustering) to uncover further patterns in the data. The goal is to explore relationships between these predictors and birth weight, using visualizations and statistical analysis in R.

------------------------------------------------------------------------

```{R, message=FALSE,results='hide', warning=FALSE}
library(tidyverse)
library(tidymodels)
library(cluster)
library(readxl)
library(readr)
library(purrr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(gmodels)
library(factoextra)
```

## Loading in Data

```{r}
Births <- read.csv("births.csv")
births_clean <- Births %>%
  mutate(
    smoke = as.factor(smoke),
    sex_baby = as.factor(sex_baby),
    premature = as.factor(premature)
  )
births_clean <- births_clean %>%
  filter(!is.na(f_age), !is.na(visits), !is.na(gained))

head(births_clean)

```

The dataset was accessed from the OpenIntro births dataset, most recently accessed on 4/28/2025 at 2:15 pm. It is a random sample of 150 births in North Carolina from 2004, which includes demographic and health-related information collected around childbirth. There are 50 observations where the mother was a smoker and another 100 observations where the mother was not. Key variables included: infant birth weight (in pounds), maternal and paternal ages, maternal weight gain during pregnancy, smoking status during pregnancy, prematurity status of the birth, and sex of the baby. I was not concerned about ethical considerations, as it was state-collected and publicly available data that was collected without a commercial, political, or private solicitation motive. With that in check, I decided to move forward with data cleaning.

## Visual summary of outcome (weight):

```{r}
ggplot(births_clean, aes(x = weight)) +
  geom_histogram(binwidth = 0.5, color = "black") +
  labs(title = "Distribution of Baby Birth Weights", x = "birth weight", y = "count") +
  theme_minimal()
```

The data is fairly normally distributed with a slight left skew

## Visualization of Birth Weight by Smoking Status:

```{r}
ggplot(births_clean, aes(x = smoke, y = weight)) + geom_boxplot(fill = "lightblue") + labs( title = "Birth Weight by Smoking Status", x = "Mother Smoked During Pregnancy", y = "Birth Weight (pounds)" ) + theme_minimal()
```

We can see that right off the bat, babies born to smoking mothers tend to weigh less than those born to nonsmoking mothers. We do see nonsmoker group has a multiple low outliers and the smoker group also has one low outlier.

Although normally outliers may be removed, I am not rushing to do so as preemie babies are usually underweight, and premature status is a variable of interest. I feel as though those points can be elaborated, and deleting them will remove important story telling portions.

## Looking at gender as a Predictor:

```{r}
births_clean %>%
  count(sex_baby) %>%
  ggplot(aes(x = sex_baby, y = n, fill = sex_baby)) +
  geom_col() +
  labs(title = "Count of Babies Sex", x = "gender", y = "count") +
  theme_minimal()

```

Although it is not a perfectly even distribution of male and female babies, the split is fairly even and thus not an area of concern

## Establishment of Models

To isolate the power of each predictor we will need a "full" model and one with more selective variables to see impacts. M1 will be full and M2 is the more selective variables of interest

Model 1 : weight \~ smoke + f_age + m_age + gained + sex_baby + premature

Model 2: weight \~ smoke+ m_age+gained+premature

```{r}
#For Reproducibility
set.seed(244)

# Define linear model specification
lm_spec <- linear_reg() %>%
  set_engine("lm")

# Perform 10-fold cross-validation using the births_clean data
births_model1_cv <- lm_spec %>%
  # fit_resamples() function is for fitting on folds
  fit_resamples(
    # Specify the relationship (Full model)
    weight ~ smoke + f_age + m_age + gained + sex_baby +premature,
    
    # vfold_cv makes CV folds randomly from births_clean data set
    resamples = vfold_cv(births_clean, v = 10),
    
    # Specify the error metrics (MAE, square root MSE, R^2)
    metrics = metric_set(mae, rmse, rsq)
  )

# Collect the average performance metrics across folds
births_model1_cv %>% collect_metrics()
```

```{r}
#I took this code from Lab 7 Intro to CV

#For Reproducibility
set.seed(244)

# Define linear model specification
lm_spec <- linear_reg() %>%
  set_engine("lm")

# Perform 10-fold cross-validation using the births_clean data
births_model2_cv <- lm_spec %>%
  # fit_resamples() function is for fitting on folds
  fit_resamples(
    # Specify the relationship 
        weight ~ smoke+ m_age+gained+premature,
    
    # vfold_cv makes CV folds randomly from births_clean data set
    resamples = vfold_cv(births_clean, v = 10),
    
    # Specify the error metrics (MAE, square root MSE, R^2)
    metrics = metric_set(mae, rmse, rsq)
  )

# Collect the average performance metrics across folds
births_model2_cv %>% collect_metrics()
```

## Using k=5 and LOOCV

```{r}
# 5-fold CV using same code as k=10 for model 1 (full)
set.seed(244)
births_model1_5 <- lm_spec %>%
  fit_resamples(
    weight ~ smoke + f_age + m_age + gained + sex_baby +premature,
    resamples = vfold_cv(births_clean, v = 5),
    metrics = metric_set(mae, rmse, rsq)
  )
births_model1_5 %>% collect_metrics()

```

```{r}
# 5-fold CV using same code as k=10 for model 2 
set.seed(244)
births_model2_5 <- lm_spec %>%
  fit_resamples(
    weight ~ smoke+ m_age+gained +premature,
    resamples = vfold_cv(births_clean, v = 5),
    metrics = metric_set(mae, rmse, rsq)
  )
births_model2_5 %>% collect_metrics()

```

```{r}
#I removed the rsq because when i included it, it ran me like an endlesss loop
births_model1_loocv <- lm_spec %>%
  fit_resamples(
    weight ~ smoke + f_age + m_age + gained + sex_baby+premature,
    resamples = vfold_cv(births_clean, v = nrow(births_clean)),
    metrics = metric_set(mae, rmse)
  )
births_model1_loocv %>% collect_metrics()

```

```{r}
#I removed the rsq because when i included it, it ran me like an endlesss loop
births_model1_loocv <- lm_spec %>%
  fit_resamples(
    weight ~ smoke + f_age + m_age + gained + sex_baby+premature,
    resamples = vfold_cv(births_clean, v = nrow(births_clean)),
    metrics = metric_set(mae, rmse)
  )
births_model1_loocv %>% collect_metrics()

```

```{r}
#I removed the rsq because when i included it, it ran me like an endlesss loop
births_model2_loocv <- lm_spec %>%
  fit_resamples(
    weight ~ smoke+ m_age+gained+premature,
    resamples = vfold_cv(births_clean, v = nrow(births_clean)),
    metrics = metric_set(mae, rmse)
  )
births_model2_loocv %>% collect_metrics()

```

Model 2 achieved lower MAE and errors across all stages, indicating a better fit to the training data. However, its 10-fold CV MAE was only marginally better than Model 1, with slightly lower standard error. Despite the slight risk of overfitting, Model 2 was chosen due to its stronger explanatory capacity and meaningful inclusion of health-related features. This reinforces the benefit of including additional predictors such as maternal age, weight gain, and premature status in improving model accuracy.

## K-Means

I thought K-means would be an interesting to see what the clusters that are formed are and what variables are driving predictive power

K-means is an unsupervised learning algorithm that partitions data into K distinct clusters. The algorithm works by minimizing the the distance between clusters by their sum of squares. The most common way to implement it is by centroids which is the total distance between each point and the center of its assigned cluster.

```{r}
#This code was adapted from K-means Lab 11
births_reduced <- births_clean %>%
  filter(!is.na(weight), !is.na(m_age), !is.na(f_age), !is.na(gained)) %>%
  select(weight, m_age, f_age, gained)

ggplot(births_reduced, aes(x = gained, y = weight)) +
  geom_point() +
  labs(title = "Initial Visualization of Baby Weight vs Weight Gained") +
  theme_minimal()

```

```{r}
set.seed(244)

# Run the K-means algorithm
kmeans_births <- kmeans(scale(births_reduced), centers = 2)

births_reduced %>%
  mutate(kmeans_cluster = as.factor(kmeans_births$cluster)) %>%
  ggplot(aes(x = gained, y = weight, color = kmeans_cluster)) +
  geom_point(size = 3) +
  theme(legend.position = "none") +
  labs(title = "K-means with K = 2 (Births Data)",
       x = "Weight Gained (lbs)",
       y = "Birth Weight (lbs)") +
  theme_minimal()

```

A PCA-based cluster plot showed interesting separation between the clusters. Cluster 1 (Red) generally included mothers with lower weight gain and lower infant birth weights. Cluster 2 (Blue) grouped pregnancies with higher maternal weight gain and heavier babies. There's some overlap in the middle (to be expected with real health data), but overall I thought K-means found two broad groups. So after everything, based on the clustering, I concluded that higher maternal weight gain tends to be associated with higher infant birth weight.

```{r}
library(factoextra)

fviz_nbclust(scale(births_reduced), kmeans, method = "silhouette") +
  labs(title = "Silhouette Method: Choosing K (Births Data)")

```

I used the silhouette method to visualize the ideal number of k clusters. This method evaluates how well each point fits within its assigned cluster compared to other clusters. The point that was highest at K=2, which indicates that two clusters provide the most meaningful separation in the data. Larger values of K did not significantly improve the structure, and sometimes even lowered cohesion.

## Conclusion:

The modeling results show that birth weight is influenced by a combination of maternal behavior (smoking), parental demographics (age), and pregnancy-related variables (weight gain, premature birth). While the simplest model generalizes slightly better, the extended model offers more insights and maintains competitive error metrics.

Clustering analysis revealed that maternal and birth characteristics naturally fall into two groups, primarily differentiated by health-related factors like weight gain and birth weight. The findings emphasize the value of integrating both supervised and unsupervised learning to interpret real-world data more holistically.
