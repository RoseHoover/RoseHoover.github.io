[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! My name is Rose Hoover, and I am a senior at Mount Holyoke College majoring in Data Science with a minor in Economics.\nI love to let statistics and data visualization guide my decision-making process, and I‚Äôm passionate about all things customer experience. I enjoy working on projects that combine code, creativity, and have real-world applications.\nIn my free time, I love to explore outdoors! You can find me rock climbing or hiking on local trails when the weather is nice\n\n\n\n\n\n\nüì´ To contact me, feel free to connect via LinkedIn\nüìÑ Or view my CV here"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Stat 244-SC",
    "section": "",
    "text": "Attached is a (TO BE COMPLETED) project from my STAT 244-Statistical Computation course at Mount Holyoke College."
  },
  {
    "objectID": "lab k means.html",
    "href": "lab k means.html",
    "title": "Lab: K-Means",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you‚Äôre working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\nFor this lab, you may need to still the package glmnet.\n\nAsk for help if you encounter issues on any of the steps above. Once you‚Äôve successfully made it through these steps, you can continue.\n\n\n\n\nLoad Packages\nYou likely will need to install some these packages before you can run the code chunk below successfully.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(factoextra)\nlibrary(amap)\n\n\n\nLoad Penguin Data\n\ndata(penguins)\n\n\n\nData Cleaning\n\n# Remove missing values\n# YOUR CODE HERE\npenguins = penguins %&gt;%\n  filter(!is.na(bill_length_mm) & !is.na(bill_depth_mm) & !is.na(species))\n# Make data table (named penguins_reduced) that only has\n# bill_length_mm and bill_depth_mm columns\npenguins_reduced &lt;- penguins %&gt;% select(bill_length_mm,bill_depth_mm)\n\n\n\nInitial Visualization\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + \n  geom_point() \n\n\n\n\n\n\n\n\nWe‚Äôll cluster these penguins based on their bill lengths and depths:\n\n\nImplement \\(K\\)-Means\nComplete the code below to run the K-means algorithm using K = 3.\n\nset.seed(244)\n# Run the K-means algorithm\nkmeans_3_round_1 &lt;- kmeans(scale(penguins_reduced), centers = 3) \n    \n# Plot the cluster assignments\npenguins_reduced %&gt;% \n  mutate(kmeans_cluster = as.factor(kmeans_3_round_1$cluster)) %&gt;%\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = kmeans_cluster)) + \n  geom_point(size = 3) + \n  theme(legend.position = \"none\") + \n  labs(title = \"K-means with K = 3 (round 1)\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWhy do we have to set the seed for K-means? In practice, why should we try out a variety of seeds?\n\nAnswer. K means is a greedy algorithm and its possible so its possible for some random locations to have better/different results each time. It is also possible for it to get stuck at a local solution\n\n\nK-Means Clusters Versus Known Species Groupings\n\n  ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + \n  geom_point(size = 3) + \n  theme(legend.position = \"none\") + \n  labs(title = \"Actual Groupings of Data Based on Species\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nVisually, how well do you think \\(K\\)-means captured the underlying species structure of the data?\n\nAnswer. It seems like it found roughly the data points corresponding to each of the species groups. It is identifying that as a way to structure the data.\n\n\nTuning \\(K\\)\n\nTo implement K-means clustering we must choose an appropriate K! Use the following example to see the two different extreme situations. Typically, the ideal \\(K\\) is somewhere between the two extremes.\nMinimum: \\(K = 2\\) groups/clusters\nMaximum: \\(K = n\\) groups/clusters (one observation per cluster)\n\nWhat happens in the \\(K\\)-means algorithm if \\(K = n\\)?\nAnswer. YOUR ANSWER HERE\nLet‚Äôs consider anywhere from \\(K = 2\\) to \\(K = 20\\) clusters.\n\nset.seed(244)\n\nk_2  &lt;- kmeans(scale(penguins_reduced), centers = 2)\nk_20 &lt;- kmeans(scale(penguins_reduced), centers = 20)\n\npenguins_reduced %&gt;% \n  mutate(cluster_2 = as.factor(k_2$cluster)) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = cluster_2)) + \n    geom_point(size = 3) + \n    labs(title = \"K = 2\")\n\n\n\n\n\n\n\npenguins_reduced %&gt;% \n  mutate(cluster_20 = as.factor(k_20$cluster)) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = cluster_20)) + \n    geom_point(size = 3) + \n    labs(title = \"K = 20\") + \n    scale_color_manual(values = rainbow(20))\n\n\n\n\n\n\n\n\nWhat are your general impressions?\nAnswer. YOUR ANSWER HERE\n\n\nFinding Ideal K Value: Silhoutte\n\nThe average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster.\n\nTo do so, it maximizes the distance between clusters and minimizes distance within clusters.\n\nA high average silhouette indicates a good clustering.\nGiven a range of possible K values, the optimal number of clusters (K) is the one that maximizes the average silhouette.\n\nWe can use a built-in silhouette method in the fviz_nbclust function to compute the average silhouette for various K values.\n\nfviz_nbclust(scale(penguins_reduced), kmeans, method='silhouette')\n\n\n\n\n\n\n\n\nBased on the average silhouette approach, what is the optimal \\(K\\) value?\nAnswer. optimal value is about 3!\n\n\nExperimenting with Distance Metrics\nWe can use the Kmeans method (notice the ‚ÄúK‚Äù is capitalized in this function name) from the amap library to specify how we are measuring distance in the K-means algorithm.\n\nset.seed(244)\nk_2_manattan = Kmeans(scale(penguins_reduced), centers = 3, \n                      method = \"manhattan\")\nk_2_euclid = Kmeans(scale(penguins_reduced), centers = 3, \n                    method = \"euclidean\")\nk_2_maxnorm = Kmeans(scale(penguins_reduced), centers = 3, \n                     method = \"maximum\")\n\n\n\nfviz_cluster(k_2_euclid, data = scale(penguins_reduced), \n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\nfviz_cluster(k_2_manattan, data = scale(penguins_reduced),\n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\nfviz_cluster(k_2_maxnorm, data = scale(penguins_reduced),\n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\n\nTry changing \\(K\\) to equal 3$ in the code chunk above. How do the clusterings using the 3 distance metrics compare? What do you generally observe?\nAnswer. YOUR ANSWER HERE\nModify the code in the chunk above so that we can easily change the value of K (rather than making sure to change K manually in every line). In general coding practices, is called extracting out a constant."
  },
  {
    "objectID": "https:/RoseHoover.github.io/index.html",
    "href": "https:/RoseHoover.github.io/index.html",
    "title": "RoseHoover.github.io",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "test.html#introduction",
    "href": "test.html#introduction",
    "title": "Stat 244-SC",
    "section": "Introduction",
    "text": "Introduction\nThis project investigates the factors that influence a newborn‚Äôs weight using a dataset that includes information on parental age, smoking status, baby sex, and whether the birth was premature. The goal is to explore relationships between these predictors and birth weight, using visualizations and statistical analysis in R.\n\n\n\nShow Code\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(cluster)\nlibrary(readxl)\nlibrary(readr)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(gmodels)\nlibrary(factoextra)"
  },
  {
    "objectID": "test.html#loading-in-data",
    "href": "test.html#loading-in-data",
    "title": "Stat 244-SC",
    "section": "Loading in Data",
    "text": "Loading in Data\n\n\nShow Code\nBirths &lt;- read.csv(\"births.csv\")\nbirths_clean &lt;- Births %&gt;%\n  mutate(\n    smoke = as.factor(smoke),\n    sex_baby = as.factor(sex_baby),\n    premature = as.factor(premature)\n  )\nbirths_clean &lt;- births_clean %&gt;%\n  filter(!is.na(f_age), !is.na(visits), !is.na(gained))\n\nhead(births_clean)\n\n\n  f_age m_age weeks premature visits gained weight sex_baby     smoke\n1    31    30    39 full term     13      1   6.88     male    smoker\n2    34    36    39 full term      5     35   7.69     male nonsmoker\n3    36    35    40 full term     12     29   8.88     male nonsmoker\n4    41    40    40 full term     13     30   9.00   female nonsmoker\n5    37    28    40 full term     12     35   8.25     male    smoker\n6    35    35    28    premie      6     29   1.63   female nonsmoker"
  },
  {
    "objectID": "test.html#visualization-birth-weight-by-smoking-status",
    "href": "test.html#visualization-birth-weight-by-smoking-status",
    "title": "Stat 244-SC",
    "section": "Visualization: Birth Weight by Smoking Status",
    "text": "Visualization: Birth Weight by Smoking Status\n\n\nShow Code\nggplot(births_clean, aes(x = smoke, y = weight)) + geom_boxplot(fill = \"lightblue\") + labs( title = \"Birth Weight by Smoking Status\", x = \"Mother Smoked During Pregnancy\", y = \"Birth Weight (pounds)\" ) + theme_minimal()\n\n\n\n\n\n\n\n\n\nWe can see that right off the bat, babies born to smoking mothers tend to weigh less than those born to nonsmoking mothers. We do see nonsmoker group has a multiple low outliers and the smoker group also has one low outlier. Although normally outliers may be removed, I am not rushing to do so as preemie babies are usually underweight, and premature status is a variable of interest. I feel as though those points can be elaborated, and deleting them will remove important story telling portions."
  },
  {
    "objectID": "test.html#visual-summary-of-outcome-weight",
    "href": "test.html#visual-summary-of-outcome-weight",
    "title": "Stat 244-SC",
    "section": "Visual summary of outcome (weight):",
    "text": "Visual summary of outcome (weight):\n\n\nShow Code\nggplot(births_clean, aes(x = weight)) +\n  geom_histogram(binwidth = 0.5, color = \"black\") +\n  labs(title = \"Distribution of Baby Birth Weights\", x = \"birth weight\", y = \"count\") +\n  theme_minimal()"
  },
  {
    "objectID": "test.html#looking-at-gender-as-a-predictor",
    "href": "test.html#looking-at-gender-as-a-predictor",
    "title": "Stat 244-SC",
    "section": "Looking at gender as a Predictor:",
    "text": "Looking at gender as a Predictor:\n\n\nShow Code\nbirths_clean %&gt;%\n  count(sex_baby) %&gt;%\n  ggplot(aes(x = sex_baby, y = n, fill = sex_baby)) +\n  geom_col() +\n  labs(title = \"Count of Babies Sex\", x = \"gender\", y = \"count\") +\n  theme_minimal()"
  },
  {
    "objectID": "test.html#establishment-of-models",
    "href": "test.html#establishment-of-models",
    "title": "Stat 244-SC",
    "section": "Establishment of Models",
    "text": "Establishment of Models\nTo isolate the power of each predictor we will need a ‚Äúfull‚Äù model and one with more selective variables to see impacts. M1 will be full and M2 is the more selective variables of interest\nModel 1 : weight ~ smoke + f_age + m_age + gained + sex_baby + premature\nModel 2: weight ~ smoke+ m_age+gained+premature\n\n\nShow Code\n#For Reproducibility\nset.seed(244)\n\n# Define linear model specification\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n# Perform 10-fold cross-validation using the births_clean data\nbirths_model1_cv &lt;- lm_spec %&gt;%\n  # fit_resamples() function is for fitting on folds\n  fit_resamples(\n    # Specify the relationship (Full model)\n    weight ~ smoke + f_age + m_age + gained + sex_baby +premature,\n    \n    # vfold_cv makes CV folds randomly from births_clean data set\n    resamples = vfold_cv(births_clean, v = 10),\n    \n    # Specify the error metrics (MAE, square root MSE, R^2)\n    metrics = metric_set(mae, rmse, rsq)\n  )\n\n# Collect the average performance metrics across folds\nbirths_model1_cv %&gt;% collect_metrics()\n\n\n# A tibble: 3 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.892    10  0.0961 Preprocessor1_Model1\n2 rmse    standard   1.12     10  0.123  Preprocessor1_Model1\n3 rsq     standard   0.322    10  0.0962 Preprocessor1_Model1\n\n\n\n\nShow Code\n#I took this code from Lab 7 Intro to CV\n\n#For Reproducibility\nset.seed(244)\n\n# Define linear model specification\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n# Perform 10-fold cross-validation using the births_clean data\nbirths_model2_cv &lt;- lm_spec %&gt;%\n  # fit_resamples() function is for fitting on folds\n  fit_resamples(\n    # Specify the relationship \n        weight ~ smoke+ m_age+gained+premature,\n    \n    # vfold_cv makes CV folds randomly from births_clean data set\n    resamples = vfold_cv(births_clean, v = 10),\n    \n    # Specify the error metrics (MAE, square root MSE, R^2)\n    metrics = metric_set(mae, rmse, rsq)\n  )\n\n# Collect the average performance metrics across folds\nbirths_model2_cv %&gt;% collect_metrics()\n\n\n# A tibble: 3 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.867    10  0.0998 Preprocessor1_Model1\n2 rmse    standard   1.08     10  0.126  Preprocessor1_Model1\n3 rsq     standard   0.336    10  0.102  Preprocessor1_Model1"
  },
  {
    "objectID": "test.html#k-means",
    "href": "test.html#k-means",
    "title": "Stat 244-SC",
    "section": "K-Means",
    "text": "K-Means\nI thought K-means would be an interesting to see what the clusters that are formed are and what variables are driving predictive power\nK-means is an unsupervised learning algorithm that partitions data into K distinct clusters. The algorithm works by minimizing the the distance between clusters by their sum of squares. The most common way to implement it is by centroids which is the total distance between each point and the center of its assigned cluster.\n\n\nShow Code\n#This code was adapted from K-means Lab 11\nbirths_reduced &lt;- births_clean %&gt;%\n  filter(!is.na(weight), !is.na(m_age), !is.na(f_age), !is.na(gained)) %&gt;%\n  select(weight, m_age, f_age, gained)\n\nggplot(births_reduced, aes(x = gained, y = weight)) +\n  geom_point() +\n  labs(title = \"Initial Visualization of Baby Weight vs Weight Gained\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nset.seed(244)\n\n# Run the K-means algorithm\nkmeans_births &lt;- kmeans(scale(births_reduced), centers = 2)\n\nbirths_reduced %&gt;%\n  mutate(kmeans_cluster = as.factor(kmeans_births$cluster)) %&gt;%\n  ggplot(aes(x = gained, y = weight, color = kmeans_cluster)) +\n  geom_point(size = 3) +\n  theme(legend.position = \"none\") +\n  labs(title = \"K-means with K = 2 (Births Data)\",\n       x = \"Weight Gained (lbs)\",\n       y = \"Birth Weight (lbs)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nlibrary(factoextra)\n\nfviz_nbclust(scale(births_reduced), kmeans, method = \"silhouette\") +\n  labs(title = \"Silhouette Method: Choosing K (Births Data)\")\n\n\n\n\n\n\n\n\n\nI used the silhouette method to vizualize the ideal number of k clusters. This method evaluates how well each point fits within its assigned cluster compared to other clusters. The point that was highest at K=2, which indicates that two clusters provide the most meaningful separation in the data. Larger values of K did not significantly improve the structure, and sometimes even lowered cohesion."
  }
]